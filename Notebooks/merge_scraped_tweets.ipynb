{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af964ea0",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook combines all the CSV files generated from the scraping process into one structured dataset. It loads, merges, and cleans the tweet data from multiple match files, ensuring that all timeline tweets and their associated replies (first-level and second-level) are aggregated into a unified format.\n",
    "\n",
    "This step is important for preparing the final dataset that will be used for toxicity detection and social network analysis. It allows me to work with the entire sample of scraped data in a consistent way, across all matches and teams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd02247e",
   "metadata": {},
   "source": [
    "### Prepare combined tweet dataset\n",
    "\n",
    "This notebook loads all tweet data collected from the scraping pipeline and merges it into a single, clean dataset for further analysis. Specifically, it brings together:\n",
    "\n",
    "- Timeline tweets from official club accounts\n",
    "- First-level replies to those tweets\n",
    "- Second-level replies to those replies (when available)\n",
    "\n",
    "The cleaning process begins by reading all individual CSV files from the `timelines`, `replies`, and `second_level_replies` folders. I loop through each folder and combine all files into a single dataframe per level. This approach allows for flexibility, as I can easily add or remove matches without changing the code.\n",
    "\n",
    "To keep track of where each tweet or reply comes from, I ensure that important metadata columns like `match_id`, `team_handle`, `tweet_url`, and `reply_url` are preserved. These identifiers are crucial for later steps:\n",
    "- In **NLP**, I will classify toxicity levels based on the `tweet` or `reply` content.\n",
    "- In **social network analysis (SNA)**, I will use these IDs to map who is interacting with whom and through what type of tweet.\n",
    "\n",
    "I also perform basic cleanup, such as:\n",
    "- Dropping duplicate tweets or replies (based on timestamp or link)\n",
    "- Normalizing column names and formats\n",
    "- Ensuring consistent datetime formats, which are important for time-based filtering or network animation\n",
    "\n",
    "After combining everything, I optionally merge the timeline tweets with the replies (if needed for the use case) or keep them separated depending on the structure required by the analysis notebooks that follow.\n",
    "\n",
    "By the end of this step, I have a centralized and standardized dataset that represents all toxic-match-related conversations. This unified dataset is the foundation for the upcoming stages of the project, including toxicity classification using NLP and influence mapping using social network analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e8b3645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load AlmereCityFC_tweets_M065_AlmereCityFC.csv: Empty or invalid file\n",
      "Failed to load PECZwolle_tweets_M053_PECZwolle.csv: Empty or invalid file\n",
      "Failed to load scHeerenveen_tweets_M015_scHeerenveen.csv: Empty or invalid file\n",
      "Failed to load scHeerenveen_tweets_M067_scHeerenveen.csv: Empty or invalid file\n",
      "Failed to load second_level_replies_M001_fcgroningen.csv: No columns to parse from file\n",
      "Failed to load second_level_replies_M007_GAEagles.csv: No columns to parse from file\n",
      "Failed to load second_level_replies_M027_RKCWAALWIJK.csv: No columns to parse from file\n",
      "Merged dataset saved to: C:/Master/Master project/merged_threads_fixed.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# === Paths ===\n",
    "timeline_path = \"C:/Master/Master project/timelines\"\n",
    "replies_path = \"C:/Master/Master project/replies\"\n",
    "second_replies_path = \"C:/Master/Master project/second_level_replies\"\n",
    "output_path = \"C:/Master/Master project/merged_threads_fixed.csv\"\n",
    "\n",
    "# === File-level functions ===\n",
    "def extract_match_and_team(filename):\n",
    "    match = re.search(r'_M(\\d+)_', filename)\n",
    "    team = filename.split(\"_\")[-1].replace(\".csv\", \"\")\n",
    "    return f\"M{match.group(1)}\" if match else None, team\n",
    "\n",
    "def safe_load_csv(filepath):\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, encoding=\"utf-8-sig\")\n",
    "        if df.empty or len(df.columns) == 0:\n",
    "            raise ValueError(\"Empty or invalid file\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {os.path.basename(filepath)}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_and_prepare(folder, depth):\n",
    "    dfs = []\n",
    "    for file in os.listdir(folder):\n",
    "        if not file.endswith(\".csv\"):\n",
    "            continue\n",
    "        full_path = os.path.join(folder, file)\n",
    "        df = safe_load_csv(full_path)\n",
    "        if df is None:\n",
    "            continue\n",
    "\n",
    "        match_id, team_handle = extract_match_and_team(file)\n",
    "        df[\"match_id\"] = match_id\n",
    "        df[\"team_handle\"] = team_handle\n",
    "        df[\"thread_depth\"] = depth\n",
    "        df[\"source_file\"] = file\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "# === Load data ===\n",
    "tweets = load_and_prepare(timeline_path, depth=0)\n",
    "replies = load_and_prepare(replies_path, depth=1)\n",
    "second = load_and_prepare(second_replies_path, depth=2)\n",
    "\n",
    "# === Clean each layer ===\n",
    "\n",
    "# --- Timeline tweets ---\n",
    "tweets = tweets.rename(columns={\"tweet\": \"text\", \"link\": \"tweet_url\"})\n",
    "tweets[\"author\"] = tweets[\"source_file\"].str.split(\"_tweets_\").str[0]\n",
    "tweets[\"parent_url\"] = None  # top-level tweets have no parent\n",
    "tweets[\"thread_id\"] = tweets[\"tweet_url\"]\n",
    "\n",
    "# --- Replies ---\n",
    "replies = replies.rename(columns={\n",
    "    \"reply\": \"text\",\n",
    "    \"reply_url\": \"tweet_url\",\n",
    "    \"in_reply_to_user\": \"original_parent_author\",  # keep original if needed\n",
    "    \"tweet_url\": \"parent_url\"  # this is the tweet being replied to\n",
    "})\n",
    "replies[\"thread_id\"] = replies[\"parent_url\"]\n",
    "\n",
    "# --- Second-level replies ---\n",
    "second = second.rename(columns={\n",
    "    \"reply\": \"text\",\n",
    "    \"reply_url\": \"tweet_url\",\n",
    "    \"parent_reply_url\": \"parent_url\",\n",
    "    \"in_reply_to_author\": \"original_parent_author\"\n",
    "})\n",
    "second[\"thread_id\"] = second[\"parent_url\"]\n",
    "\n",
    "# === Ensure all columns exist ===\n",
    "all_cols = [\n",
    "    \"match_id\", \"team_handle\", \"thread_depth\", \"thread_id\", \"author\",\n",
    "    \"text\", \"timestamp\", \"tweet_url\", \"parent_author\", \"parent_url\",\n",
    "    \"comments\", \"source_file\", \"original_parent_author\"\n",
    "]\n",
    "\n",
    "def fill_missing_columns(df):\n",
    "    for col in all_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    return df[all_cols]\n",
    "\n",
    "tweets = fill_missing_columns(tweets)\n",
    "replies = fill_missing_columns(replies)\n",
    "second = fill_missing_columns(second)\n",
    "\n",
    "# === Merge all ===\n",
    "merged = pd.concat([tweets, replies, second], ignore_index=True)\n",
    "merged = merged.sort_values(by=[\"match_id\", \"thread_id\", \"timestamp\", \"thread_depth\"]).reset_index(drop=True)\n",
    "\n",
    "# === Rebuild parent_author using tweet_url → author map\n",
    "url_to_author = merged.set_index(\"tweet_url\")[\"author\"].to_dict()\n",
    "merged[\"parent_author\"] = merged[\"parent_url\"].map(url_to_author)\n",
    "\n",
    "# === Fallback to scraped in_reply_to when mapping fails\n",
    "merged[\"parent_author\"] = merged[\"parent_author\"].fillna(merged[\"original_parent_author\"])\n",
    "\n",
    "# === Save output ===\n",
    "merged.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Merged dataset saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fd6b88",
   "metadata": {},
   "source": [
    "### Note on empty or missing files\n",
    "\n",
    "During the merge process, you may see warnings such as:\n",
    "\n",
    "- `Empty or invalid file`\n",
    "- `No columns to parse from file`\n",
    "\n",
    "These warnings occur when the scraper did not generate any tweets or replies for a particular match or team. This is not an error — it simply means that:\n",
    "\n",
    "- No tweets were posted by that account in the scraping window, or\n",
    "- No tweets received replies, or\n",
    "- Nitter failed to load valid tweet content during scraping\n",
    "\n",
    "The merge step skips over these files and continues processing the rest of the data. These warnings can be safely ignored and do not affect the functionality of the tool.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Introduction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
